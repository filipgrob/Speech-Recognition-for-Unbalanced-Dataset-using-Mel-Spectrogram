{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbcb136e",
   "metadata": {},
   "source": [
    "# DSL winter project 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceb8cb2",
   "metadata": {},
   "source": [
    "### Bianco Matteo s300781, Grobbo Filippo s305723"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d3f88f",
   "metadata": {},
   "source": [
    "## Import packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555afe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import spectrogram\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import skew, kurtosis\n",
    "import noisereduce as nr\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865d6c7",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db83291",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dsl_data/development.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a46004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of new columns 'label' containing samples labels (concatenating columns 'action' and 'object')\n",
    "\n",
    "df['label'] = df['action']+df['object']\n",
    "df.drop(columns=['action','object'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c605e67f",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting cardinality of each class\n",
    "\n",
    "np.unique(df['label'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b5f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random oversampling\n",
    "\n",
    "col_names = df.columns\n",
    "y = df['label'].values\n",
    "X = df.iloc[:,:-1].values\n",
    "ros = RandomOverSampler(random_state=42, sampling_strategy='not majority') #resample all classes but the majority class\n",
    "X_resampled, y_resampled = ros.fit_resample(X,y)\n",
    "df_resampled = pd.DataFrame(np.hstack((X_resampled,y_resampled.reshape(-1,1))), columns=col_names)\n",
    "df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35e9321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle rows and reset indeces\n",
    "\n",
    "df_resampled = df_resampled.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121804ed",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd78477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import audio wav files as numpy arrays\n",
    "\n",
    "audio_list_res = []\n",
    "sample_rates_res = []\n",
    "for path in df_resampled['path']:\n",
    "    file_read = scipy.io.wavfile.read(path)\n",
    "    sample_rates_res.append(file_read[0])\n",
    "    audio_list_res.append(file_read[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b53ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check audio encoding\n",
    "\n",
    "max_list_res=[]\n",
    "for audio in audio_list_res:\n",
    "    max_list_res.append(max(audio))\n",
    "max(max_list_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8324a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check sample rates different values\n",
    "\n",
    "np.unique(sample_rates_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fdc86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute mean absolute amplitude value of all audios (in a first moment we used this value as a trimming threshold)\n",
    "\n",
    "mean = np.concatenate([np.abs(audio) for audio in audio_list_res]).mean()\n",
    "mean  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac93d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trimming audios to eliminate silence sections\n",
    "\n",
    "threshold = 500  #we finally decided to use this threshold\n",
    "#initialization of list and indeces\n",
    "audio_trim=[]\n",
    "drop_list=[]\n",
    "i=0\n",
    "for audio in tqdm(audio_list_res):  #scroll all audio in the list\n",
    "    #initialization\n",
    "    index_first=0\n",
    "    index_last=-2\n",
    "    #Find indeces of values above threshold\n",
    "    indexes_above=np.where(np.abs(audio)>threshold)[0]\n",
    "    #Check if there are values above the threshold\n",
    "    if len(indexes_above)!=0:\n",
    "        index_first=indexes_above[0]\n",
    "        index_last=indexes_above[-1]\n",
    "    else:\n",
    "        drop_list.append(i)  #REM: audio made just of white noise ar not trimmed, we'll drop them later\n",
    "    #i is the index of audios\n",
    "    i=i+1\n",
    "    #creation of trimmed audio\n",
    "    audio_trim.append(audio[index_first:index_last+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c83c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop white noise audios\n",
    "\n",
    "df_nonoise = df_resampled.drop(index=drop_list)\n",
    "\n",
    "for index in sorted(drop_list, reverse=True):\n",
    "    del audio_trim[index]\n",
    "    del audio_list_res[index]\n",
    "    del sample_rates_res[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32ce89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noise reduction of audios\n",
    "\n",
    "audio_trim = [nr.reduce_noise(audio_trim[i], sample_rates_res[i], n_fft=1024, hop_length=256) for i in tqdm(range(len(audio_trim)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a763065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Mel-spectrogram of each audio\n",
    "\n",
    "audio_spectr = []\n",
    "for audio, sr in tqdm(zip(audio_trim, sample_rates_res), total=len(audio_trim)):\n",
    "    spectrogram = librosa.feature.melspectrogram(y=audio.astype(float), sr = sr, n_fft=1024, hop_length=256)\n",
    "    #Converting power units in deciBel scale\n",
    "    audio_spectr.append(librosa.power_to_db(spectrogram, ref=np.max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ad612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature extraction from Mel-spectrograms\n",
    "\n",
    "#number of blocks hyperparameters\n",
    "num_col = 6  \n",
    "num_row = 16\n",
    "row_width = int(128/num_row)\n",
    "num_chunks = num_col\n",
    "#initialization of features matrix\n",
    "df_arr = np.zeros((len(audio_spectr), 6*num_col*num_row))\n",
    "#Loop on each Mel-spectrogram\n",
    "for index, spectr in tqdm(enumerate(audio_spectr), total = len(audio_spectr)):\n",
    "    #Initialization of matrices relative to each statistic to calculate for every spectrogram\n",
    "    col_chunks = np.array_split(spectr, num_chunks, axis=1)\n",
    "    features_m = np.zeros((num_row,num_col))\n",
    "    features_s = np.zeros((num_row,num_col))\n",
    "    features_max = np.zeros((num_row,num_col))\n",
    "    features_min = np.zeros((num_row,num_col))\n",
    "    features_1quart = np.zeros((num_row,num_col))\n",
    "    features_3quart = np.zeros((num_row,num_col))\n",
    "    #Loop on each block in which we divide the spectrogram\n",
    "    for j, chunk in enumerate(col_chunks):\n",
    "        for i in range(num_row):\n",
    "            sub_mat = chunk[row_width*i:row_width*(i+1),:]\n",
    "            #Computation of statistics\n",
    "            features_m[i,j] = sub_mat.mean()\n",
    "            features_s[i,j] = sub_mat.std()\n",
    "            features_max[i,j] = sub_mat.max()\n",
    "            features_min[i,j] = sub_mat.min()\n",
    "            features_1quart[i,j] = np.quantile(sub_mat.reshape(-1), q=0.25)\n",
    "            features_3quart[i,j] = np.quantile(sub_mat.reshape(-1), q=0.75)\n",
    "    #Grouping features together\n",
    "    features = np.concatenate((features_m.reshape(-1), features_s.reshape(-1), features_max.reshape(-1), features_min.reshape(-1), features_1quart.reshape(-1), features_3quart.reshape(-1)))\n",
    "    #Assigning features relative to each audio to the feature matrix\n",
    "    df_arr[index,:] = features.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bfab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of DataFrame containing features extracted from Mel-spectrograms\n",
    "\n",
    "#Define columns names\n",
    "df_col=list(df_resampled.columns)\n",
    "for j in range(num_col):\n",
    "    for i in range(num_row):\n",
    "        df_col=df_col+[f'mean_{i,j}']+[f'std_{i,j}']+[f'max_{i,j}']+[f'min_{i,j}']+[f'1quart_{i,j}']+[f'3quart_{i,j}']\n",
    "\n",
    "df_spectr = pd.DataFrame(index = range(len(df_nonoise.index)), columns=df_col)\n",
    "#We assign to each sample also the fetures contained in the original dataframe\n",
    "df_spectr.iloc[:,:9] = df_nonoise.values\n",
    "#We assign to each sample the new features computed on Mel-spectrograms\n",
    "df_spectr.iloc[:,9:] = df_arr\n",
    "df_spectr.columns = df_col\n",
    "\n",
    "df_spectr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3c0198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add feature with time duration for each audio\n",
    "\n",
    "df_spectr['duration'] = [len(audio)/rate for audio, rate in zip(audio_trim, sample_rates_res)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30850a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot Encoding of feature \"gender\"\n",
    "\n",
    "df_encoded = pd.get_dummies(df_spectr, columns=list(df_spectr.columns[[6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867b493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify col name of feature 'Self-reported fluency level' beacuse it gives issues in accessing it\n",
    "\n",
    "col_names = list(df_encoded.columns)\n",
    "col_names[3] = 'Self reported fluency level'\n",
    "df_encoded.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd541f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check unique values of categorical features 'Self reported fluency level' and 'ageRange'\n",
    "\n",
    "df_encoded['Self reported fluency level'].unique(), df_encoded['ageRange'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c26bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encoding of features 'Self reported fluency level' and 'ageRange'\n",
    "\n",
    "df_encoded.replace({'Self reported fluency level': {'basic':1, 'intermediate':2, 'advanced':3, 'native':4},\\\n",
    "                    'ageRange':{'22-40':1, '41-65':2, '65+':3}},\\\n",
    "                   inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc344d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop and save in another variable the column containing audio labels\n",
    "\n",
    "label = df_encoded['label']\n",
    "\n",
    "#Drop features which resulted with low feature importance (with Random Forest) in preliminary analysis\n",
    "\n",
    "df_encoded.drop(columns=['Id', 'path', 'speakerId', 'First Language spoken', 'Current language used for work/school','label'], inplace=True)\n",
    "df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68605f0",
   "metadata": {},
   "source": [
    "## Grid seach for model selection and hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45f0e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature normalization for SVM (run only for SVM)\n",
    "\n",
    "scaler_SVM = StandardScaler()\n",
    "X_scaled = scaler_SVM.fit_transform(df_encoded.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396eac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gridsearch with SVM\n",
    "\n",
    "clf = SVC(random_state=42)\n",
    "params = {'C':(1,5,10,50), 'degree':(3,5,7), 'kernel':('poly', 'rbf')}\n",
    "gs_over = GridSearchCV(estimator = clf, param_grid = params, scoring = 'accuracy', verbose=10)\n",
    "gs_over.fit(X_scaled, label)\n",
    "pd.DataFrame(gs_over.cv_results_).sort_values(by=['rank_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ce6344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save in csv file results of SVM grid search\n",
    "\n",
    "gs_over_df = pd.DataFrame(gs_over.cv_results_).sort_values(by=['rank_test_score'])\n",
    "gs_over_df.to_csv(path_or_buf='grid_search_over_16rows_minmax.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ba82b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gridsearch with RandomForest\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "params = {'n_estimators':(160,170,180), 'max_samples':(0.7,0.8), 'min_samples_split':(2,4)}\n",
    "gs_rf = GridSearchCV(estimator = clf, param_grid = params, scoring = 'accuracy', verbose=10)\n",
    "gs_rf.fit(df_encoded.values, label)\n",
    "pd.DataFrame(gs_rf.cv_results_).sort_values(by=['rank_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc9e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save in csv file results of Random Forest grid search\n",
    "\n",
    "gs_over_df = pd.DataFrame(gs_rf.cv_results_).sort_values(by=['rank_test_score'])\n",
    "gs_over_df.to_csv(path_or_buf='grid_search_over_RF_32rows.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6775a88f",
   "metadata": {},
   "source": [
    "## Test best SVM using larger values for hyperparameter C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f479647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gridsearch with best SVM to test if bigger values of C may improve performances\n",
    "\n",
    "clf = SVC(kernel = 'rbf', random_state=42)\n",
    "params = {'C':(100, 200, 500)}\n",
    "gs_over = GridSearchCV(estimator = clf, param_grid = params, scoring = 'accuracy', verbose=10)\n",
    "gs_over.fit(X_scaled, label)\n",
    "pd.DataFrame(gs_over.cv_results_).sort_values(by=['rank_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15586e7b",
   "metadata": {},
   "source": [
    "## Plots for the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12563692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import audio wav files from the original dataset\n",
    "\n",
    "audio_list = []\n",
    "sample_rates = []\n",
    "for path in df['path']:\n",
    "    file_read = scipy.io.wavfile.read(path)\n",
    "    sample_rates.append(file_read[0])\n",
    "    audio_list.append(file_read[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7100fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of time durations of audios\n",
    "\n",
    "duration = []\n",
    "for audio, rate in zip(audio_list, sample_rates):\n",
    "    duration.append(len(audio)/rate)\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ax.hist(duration, bins=50)\n",
    "ax.set_xlabel('Duration (s)')\n",
    "ax.set_ylabel('Number of audios')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "#plt.savefig('duration_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829488ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look for a proper audio file to use for the next plot \n",
    "\n",
    "index = duration.index(20, 616)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78af546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the chosen audio in time domain\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ax.plot(audio_list[index])\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Amplitude (16-bit encoding)')\n",
    "ax.set_xticks([i*sample_rates[index] for i in range(0,25,5)])\n",
    "ax.set_xticklabels([0,5,10,15,20])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "#plt.savefig('audio_20s.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708f4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find where the chosen audio is trimmed, for the next plot\n",
    "\n",
    "threshold = 500\n",
    "audio = audio_list[index]\n",
    "index_first=0\n",
    "index_last=-2\n",
    "#first index\n",
    "indexes_above=np.where(np.abs(audio)>threshold)[0]\n",
    "if len(indexes_above)!=0:\n",
    "    index_first=indexes_above[0]\n",
    "    index_last=indexes_above[-1]\n",
    "else:\n",
    "    drop_list.append(i) \n",
    "    #creation of trimmed audio\n",
    "audio_trimmed = audio[index_first:index_last+1] #We also trim the audio for spectrogram plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b375a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot chosen audio in time domain, adding lines showing where it is trimmed\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ax.plot(audio_list[index])\n",
    "ax.vlines([index_first, index_last], ymin=min(audio_list[index]), ymax=max(audio_list[index]), colors='red', linestyles='dashed', label='Where audio is cut')\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Amplitude (16-bit encoding)')\n",
    "ax.set_xticks([i*sample_rates[index] for i in range(0,25,5)])\n",
    "ax.set_xticklabels([0,5,10,15,20])\n",
    "ax.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "#plt.savefig('audio_trim.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b536013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply noise reduction and compute Mel-spectrogram of the chosen audio\n",
    "\n",
    "audio_nr = nr.reduce_noise(audio_trimmed, sample_rates[index], n_fft=1024, hop_length=256)\n",
    "spectrogram = librosa.feature.melspectrogram(y=audio_nr.astype(float), sr = sample_rates[index], n_fft=1024, hop_length=256)\n",
    "audio_spectrogram = librosa.power_to_db(spectrogram, ref=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ce9d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Mel-spectrogram of the chosen audio\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "img = librosa.display.specshow(audio_spectrogram, x_axis='time',\n",
    "                               y_axis='mel', sr=sample_rates[index], ax=ax)\n",
    "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "plt.show()\n",
    "#plt.savefig('Mel_spectrogram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08016913",
   "metadata": {},
   "source": [
    "## Applying all preprocessing steps to evaluation set for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a64d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import evaluation set\n",
    "\n",
    "df_eval = pd.read_csv('dsl_data/evaluation.csv')\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba36b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import audio files of evaluation set\n",
    "\n",
    "audio_list_eval = []\n",
    "sample_rates_eval = []\n",
    "for path in df_eval['path']:\n",
    "    file_read = scipy.io.wavfile.read(path)\n",
    "    sample_rates_eval.append(file_read[0])\n",
    "    audio_list_eval.append(file_read[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d2e36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying trimming procedure\n",
    "\n",
    "threshold=500\n",
    "audio_trim_eval=[]\n",
    "drop_list_eval=[]\n",
    "i=0\n",
    "for audio in tqdm(audio_list_eval):\n",
    "    index_first=0\n",
    "    index_last=-2\n",
    "    #first index\n",
    "    indexes_above=np.where(np.abs(audio)>threshold)[0]\n",
    "    if len(indexes_above)!=0:\n",
    "        index_first=indexes_above[0]\n",
    "        index_last=indexes_above[-1]\n",
    "    else:\n",
    "        drop_list_eval.append(i)\n",
    "    #i is the index of audios\n",
    "    i=i+1\n",
    "    #creation of trimmed audio\n",
    "    audio_trim_eval.append(audio[index_first:index_last+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618a9b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we see that on the evaluation set no audio is dropped (what happens to audio which are made of only noise)\n",
    "\n",
    "drop_list_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18cdf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noise reduction on evaluation set\n",
    "\n",
    "audio_trim_eval = [nr.reduce_noise(audio_trim_eval[i], sample_rates_eval[i], n_fft=1024, hop_length=256) for i in tqdm(range(len(audio_trim_eval)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ec72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spectrograms calculation of evaluation set\n",
    "\n",
    "audio_spectr_eval = []\n",
    "for audio, sr in tqdm(zip(audio_trim_eval, sample_rates_eval), total=len(audio_trim_eval)):\n",
    "    spectrogram = librosa.feature.melspectrogram(y=audio.astype(float), sr = sr, n_fft=512, hop_length=256)\n",
    "    audio_spectr_eval.append(librosa.power_to_db(spectrogram, ref=np.max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d433702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features extraction from evaluation set spectrograms\n",
    "\n",
    "num_col = 6\n",
    "num_row = 16\n",
    "row_width = int(128/num_row)\n",
    "num_chunks = num_col\n",
    "df_arr_eval = np.zeros((len(audio_spectr_eval), 6*num_col*num_row))\n",
    "for index, spectr in tqdm(enumerate(audio_spectr_eval), total = len(audio_spectr_eval)):\n",
    "    col_chunks = np.array_split(spectr, num_chunks, axis=1)\n",
    "    features_m = np.zeros((num_row,num_col))\n",
    "    features_s = np.zeros((num_row,num_col))\n",
    "    features_max = np.zeros((num_row,num_col))\n",
    "    features_min = np.zeros((num_row,num_col))\n",
    "    features_1quart = np.zeros((num_row,num_col))\n",
    "    features_3quart = np.zeros((num_row,num_col))\n",
    "    for j, chunk in enumerate(col_chunks):\n",
    "        for i in range(num_row):\n",
    "            sub_mat = chunk[row_width*i:row_width*(i+1),:]\n",
    "            features_m[i,j] = sub_mat.mean()\n",
    "            features_s[i,j] = sub_mat.std()\n",
    "            features_max[i,j] = max(sub_mat.reshape(-1))\n",
    "            features_min[i,j] = min(sub_mat.reshape(-1))\n",
    "            features_1quart[i,j] = np.quantile(sub_mat.reshape(-1), q=0.25)\n",
    "            features_3quart[i,j] = np.quantile(sub_mat.reshape(-1), q=0.75)\n",
    "    features = np.concatenate((features_m.reshape(-1), features_s.reshape(-1), features_max.reshape(-1), features_min.reshape(-1), features_1quart.reshape(-1), features_3quart.reshape(-1)))\n",
    "    df_arr_eval[index,:] = features.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c6b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of a dataset containing evaluation set features\n",
    "\n",
    "df_col_eval=list(df_eval.columns)\n",
    "for j in range(num_col):\n",
    "    for i in range(num_row):\n",
    "        df_col_eval=df_col_eval+[f'mean_{i,j}']+[f'std_{i,j}']+[f'max_{i,j}']+[f'min_{i,j}']+[f'1quart_{i,j}']+[f'3quart_{i,j}']       \n",
    "            \n",
    "df_spectr_eval = pd.DataFrame(index = range(len(df_eval.index)), columns=df_col_eval)\n",
    "df_spectr_eval.iloc[:,:8] = df_eval.values\n",
    "df_spectr_eval.iloc[:,8:] = df_arr_eval\n",
    "\n",
    "df_spectr_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99c542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add feature with time duration\n",
    "\n",
    "df_spectr_eval['duration'] = [len(audio)/rate for audio, rate in zip(audio_trim_eval, sample_rates_eval)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee67c62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encoding of feature 'gender'\n",
    "\n",
    "df_enceval = pd.get_dummies(df_spectr_eval, columns=list(df_spectr_eval.columns[[6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5107789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify name of column 'Self-reported fluency level'\n",
    "\n",
    "col_names = list(df_enceval.columns)\n",
    "col_names[3] = 'Self reported fluency level'\n",
    "df_enceval.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c424f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encoding of features 'Self reported fluency level' and 'ageRange'\n",
    " \n",
    "df_enceval.replace({'Self reported fluency level': {'basic':1, 'intermediate':2, 'advanced':3, 'native':4},\\\n",
    "                    'ageRange':{'22-40':1, '41-65':2, '65+':3}},\\\n",
    "                   inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa731e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop features which resulted with low feature importance (with Random Forest) in preliminary analysis\n",
    "\n",
    "df_enceval.drop(columns=['Id', 'path', 'speakerId', 'First Language spoken', 'Current language used for work/school'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bfdd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize data according to scale of training set (only for SVM)\n",
    "\n",
    "X_eval_scaled = scaler_SVM.transform(df_enceval.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813c8efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submission with Random Forest grid search best result (obtained on training set)\n",
    "\n",
    "y_pred = gs_rf.best_estimator_.predict(df_enceval.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c7343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submission with SVM grid search best result (obtained on training set)\n",
    "\n",
    "y_pred = gs_over.best_estimator_.predict(X_eval_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d2fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of dataframe with predicted labels\n",
    "\n",
    "ris = pd.DataFrame({'Id':df_enceval.index, 'Predicted': y_pred})\n",
    "\n",
    "#Save dataframe to csv file (in order to submit to leaderboard)\n",
    "\n",
    "ris.to_csv(path_or_buf='result_exam23.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
